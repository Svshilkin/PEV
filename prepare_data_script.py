import pandas as pd
import numpy as np
import csv
import re
import matplotlib.pyplot as plt
import os
import warnings
from scipy import stats

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("‚ö†Ô∏è psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install psutil")

warnings.filterwarnings('ignore')

def from_raw(file, chunk_size=50000):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
    cooked = 'vacancies_cooked.csv'

    # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –∏–ª–∏ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ)
    columns_to_remove = {
        'driver_licence_e', 'accommodation_housing', 'disabled', 'inner_info_metro_ids',
        'retraining_grant', 'date_time_change_inner_info', 'payment_meals',
        'retraining_capability', 'caring_workers', 'federal_district',
        'payment_sports_activities', 'job_benefits', 'retraining_condition',
        'additional_info', 'metro_station', 'incentive_compensation_transport_compensation',
        'premium_size', 'driver_licence_c', 'driver_licence_b', 'premium_type',
        'career_perspective', 'workers_with_disabled_children', 'dms', 'released_persons',
        'driver_licence_d', 'drive_licences', 'social_protecteds_social_protected',
        'single_parent', 'vouchers_health_institutions', 'retraining_grant_value',
        'requirements_id_priority_category', 'need_medcard', 'driver_licence_a',
        'vac_url', 'accommodation_capability', 'is_uzbekistan_recruitment'
    }

    print("üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º)...")
    engineer_count = 0
    total_count = 0
    chunk_count = 0
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å CSV —Ñ–∞–π–ª–∞
    delimiter = ';'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
    with open(file, 'r', newline='', encoding='utf-8') as infile:
        reader = csv.reader(infile, delimiter=delimiter)
        header = next(reader)

    print(f"üìã –ù–∞–π–¥–µ–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ: {len(header)}")
    print(f"üìã –ü–µ—Ä–≤—ã–µ 10 —Å—Ç–æ–ª–±—Ü–æ–≤: {header[:10]}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ title
    if 'title' not in header:
        print(f"‚ùå –°—Ç–æ–ª–±–µ—Ü 'title' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print(f"üìã –í—Å–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —Ñ–∞–π–ª–µ: {header}")
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–æ–ª–±—Ü—ã
        title_like = [col for col in header if 'title' in col.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in col.lower() or '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' in col.lower()]
        if title_like:
            print(f"üîç –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–æ–ª–±—Ü—ã: {title_like}")
        raise SystemExit('‚ùå –°—Ç–æ–ª–±–µ—Ü "title" –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö!')

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    keep_indices = [i for i, col in enumerate(header) if col not in columns_to_remove]
    new_header = [header[i] for i in keep_indices]

    print(f"üìã –°—Ç–æ–ª–±—Ü–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(new_header)}")

    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Å—Ç–æ–ª–±—Ü–∞ "title"
    try:
        title_index = new_header.index('title')
        print(f"‚úÖ –°—Ç–æ–ª–±–µ—Ü 'title' –Ω–∞–π–¥–µ–Ω –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏: {title_index}")
    except ValueError:
        print(f"‚ùå –°—Ç–æ–ª–±–µ—Ü 'title' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏!")
        print(f"üìã –°—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {new_header}")
        raise SystemExit('‚ùå –°—Ç–æ–ª–±–µ—Ü "title" –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!')

    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    with open(cooked, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.writer(outfile, delimiter=';')
        writer.writerow(new_header)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º
        with open(file, 'r', newline='', encoding='utf-8') as infile:
            reader = csv.reader(infile, delimiter=delimiter)
            next(reader)  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            
            chunk_data = []
            for row in reader:
                total_count += 1
                chunk_data.append(row)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫ –∫–æ–≥–¥–∞ –æ–Ω –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if len(chunk_data) >= chunk_size:
                    chunk_count += 1
                    engineer_found = process_raw_chunk(chunk_data, keep_indices, title_index, writer)
                    engineer_count += engineer_found
                    chunk_data = []

                    
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                    import gc
                    gc.collect()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ
            if chunk_data:
                chunk_count += 1
                engineer_found = process_raw_chunk(chunk_data, keep_indices, title_index, writer)
                engineer_count += engineer_found
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ {chunk_count}: {total_count:,} —Å—Ç—Ä–æ–∫, –Ω–∞–π–¥–µ–Ω–æ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö: {engineer_count:,}")

    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_count:,} –∑–∞–ø–∏—Å–µ–π")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {engineer_count:,} –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π ({engineer_count/total_count*100:.1f}%)")
    
    return cooked

def process_raw_chunk(chunk_data, keep_indices, title_index, writer):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    engineer_count = 0
    
    for row in chunk_data:
        new_row = [row[i] if i < len(row) else '' for i in keep_indices]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ "–∏–Ω–∂–µ–Ω–µ—Ä" –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        if title_index < len(new_row) and '–∏–Ω–∂–µ–Ω–µ—Ä' in new_row[title_index].lower():
            writer.writerow(new_row)
            engineer_count += 1
    
    return engineer_count

def normalize_workplaces(value):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç"""
    if pd.isna(value):
        return 1

    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–∏–¥–∞ "–æ—Ç X –¥–æ Y"
        if isinstance(value, str) and '–¥–æ' in value:
            match = re.search(r'–æ—Ç\s+(\d+)\s+–¥–æ\s+(\d+)', value)
            if match:
                min_val = int(match.group(1))
                max_val = int(match.group(2))
                return (min_val + max_val) // 2
        elif isinstance(value, str) and '–æ—Ç' in value:
            match = re.search(r'–æ—Ç\s+(\d+)', value)
            if match:
                return int(match.group(1))
    except:
        pass

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
    try:
        result = max(1, int(float(value)))
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
        return min(result, 10000)  
    except:
        return 1

def encode_category_by_target(df, feature, target='work_places', drop_original=False):
    """
    –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
    """
    if feature not in df.columns:
        print(f"‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫ {feature} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö")
        return df

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    df[feature] = df[feature].fillna('Unknown')

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∑–∞–º–µ–Ω—ã —Å —É—á–µ—Ç–æ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    category_stats = df.groupby(feature)[target].agg(['mean', 'count']).reset_index()
    global_mean = df[target].mean()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
    min_samples = 10
    encoding_dict = {}
    
    for _, row in category_stats.iterrows():
        category = row[feature]
        category_mean = row['mean']
        category_count = row['count']
        
        if category_count < min_samples:
            # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –ø–æ –õ–∞–ø–ª–∞—Å—É
            alpha = min_samples
            smoothed_mean = (category_mean * category_count + global_mean * alpha) / (category_count + alpha)
            encoding_dict[category] = smoothed_mean
        else:
            encoding_dict[category] = category_mean

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    df[f'{feature}_encoded'] = df[feature].map(lambda x: encoding_dict.get(x, global_mean))

    # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if drop_original and feature != target:
        df = df.drop(feature, axis=1)

    return df

def discretize_numeric_feature(df, feature, num_bins=10, target='work_places', drop_original=False):
    """
    –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –±–∏–Ω–∞–º–∏.
    """
    if feature not in df.columns:
        print(f"‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫ {feature} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö")
        return df

    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ
        if not pd.api.types.is_numeric_dtype(df[feature]):
            df[feature] = pd.to_numeric(df[feature], errors='coerce')

        # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã –ø–µ—Ä–µ–¥ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–µ–π
        Q1 = df[feature].quantile(0.25)
        Q3 = df[feature].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
        df[feature] = df[feature].clip(lower_bound, upper_bound)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π
        df[feature] = df[feature].fillna(df[feature].median())

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—É—é –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        try:
            df[f'{feature}_bin_idx'], bin_edges = pd.qcut(df[feature], q=num_bins, 
                                                         labels=False, duplicates='drop', retbins=True)
        except ValueError:
            # –ï—Å–ª–∏ –∫–≤–∞–Ω—Ç–∏–ª—å–Ω–∞—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            df[f'{feature}_bin_idx'], bin_edges = pd.cut(df[feature], bins=num_bins, 
                                                        labels=False, retbins=True)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∏–Ω–∞
        bin_means = {}
        global_mean = df[target].mean()
        
        for bin_idx in range(len(bin_edges) - 1):
            mask = df[f'{feature}_bin_idx'] == bin_idx
            if mask.any() and mask.sum() > 0:
                bin_means[bin_idx] = df.loc[mask, target].mean()
            else:
                bin_means[bin_idx] = global_mean

        # –°–æ–∑–¥–∞–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        df[f'{feature}_encoded'] = df[f'{feature}_bin_idx'].map(bin_means)
        df[f'{feature}_encoded'] = df[f'{feature}_encoded'].fillna(global_mean)

        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
        df = df.drop(f'{feature}_bin_idx', axis=1)

        # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if drop_original and feature != target:
            df = df.drop(feature, axis=1)

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ {feature}: {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–ª–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        df[f'{feature}_encoded'] = df[target].mean()

        if drop_original and feature != target:
            df = df.drop(feature, axis=1)

    return df

def process_date_feature(df, date_col, mistake_col=None, drop_original=False):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞—Ç.
    """
    if date_col not in df.columns:
        print(f"‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫ {date_col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö")
        return df

    # –ö–æ–ø–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    df[f'{date_col}_processed'] = df[date_col].copy()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫
    if mistake_col and mistake_col in df.columns:
        df.loc[df[mistake_col] == '1', f'{date_col}_processed'] = None

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime
    df[f'{date_col}_processed'] = pd.to_datetime(df[f'{date_col}_processed'], errors='coerce')

    # –ï—Å–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if not df[f'{date_col}_processed'].isna().all():
        # –û—Å–Ω–æ–≤–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df[f'{date_col}_year'] = df[f'{date_col}_processed'].dt.year
        df[f'{date_col}_month'] = df[f'{date_col}_processed'].dt.month
        df[f'{date_col}_quarter'] = df[f'{date_col}_processed'].dt.quarter
        df[f'{date_col}_day_of_week'] = df[f'{date_col}_processed'].dt.dayofweek
        df[f'{date_col}_day_of_year'] = df[f'{date_col}_processed'].dt.dayofyear

        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for temp_feature in [f'{date_col}_year', f'{date_col}_month', f'{date_col}_quarter']:
            df = encode_category_by_target(df, temp_feature, drop_original=False)

    # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if drop_original:
        if date_col in df.columns:
            df = df.drop(date_col, axis=1)
        if mistake_col and mistake_col in df.columns:
            df = df.drop(mistake_col, axis=1)

    return df

def process_chunk(chunk):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç
        chunk['work_places'] = chunk['work_places'].apply(normalize_workplaces)
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
        date_columns = [
            ('date_creation', 'date_creation_mistake'),
            ('date_posted', 'date_posted_mistake'),
            ('date_inactivation', None),
            ('date_last_updated', None),
            ('date_modify_inner_info', 'date_modify_inner_info_mistake')
        ]

        for date_col, mistake_col in date_columns:
            if date_col in chunk.columns:
                try:
                    chunk = process_date_feature(chunk, date_col, mistake_col, drop_original=False)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞—Ç—ã {date_col}: {e}")
                    continue

        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç—Ä–∞—Å–ª–∏
        if 'industry' in chunk.columns:
            try:
                chunk['industry'] = chunk['industry'].fillna('Unknown')
                chunk = encode_category_by_target(chunk, 'industry', drop_original=False)
            except Exception as e:
                print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç—Ä–∞—Å–ª–∏: {e}")

        categorical_features = [
            'employment_type', 'education_requirements_education_type',
            'region', 'profession', 'organization'
        ]

        for feature in categorical_features:
            if feature in chunk.columns:
                try:
                    chunk = encode_category_by_target(chunk, feature, drop_original=False)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {feature}: {e}")
                    continue

        # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_features = [
            'base_salary_min', 'base_salary_max', 'experience_requirements'
        ]

        for feature in numeric_features:
            if feature in chunk.columns:
                try:
                    chunk = discretize_numeric_feature(chunk, feature, drop_original=False)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {feature}: {e}")
                    continue

        # 5. –ö–õ–Æ–ß–ï–í–ê–Ø –õ–û–ì–ò–ö–ê: –°–æ–∑–¥–∞–Ω–∏–µ date_end –∏ vacancy_duration (–ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ improved_solution.py)
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏–∏
            if 'date_inactivation_processed' in chunk.columns and 'date_last_updated_processed' in chunk.columns:
                chunk['date_end'] = chunk['date_inactivation_processed'].fillna(chunk['date_last_updated_processed'])
            elif 'date_last_updated_processed' in chunk.columns:
                chunk['date_end'] = chunk['date_last_updated_processed']
            elif 'date_inactivation_processed' in chunk.columns:
                chunk['date_end'] = chunk['date_inactivation_processed']
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–∫–æ–Ω—á–∞–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω—é—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                if 'date_creation_processed' in chunk.columns:
                    chunk['date_end'] = chunk['date_creation_processed'] + pd.Timedelta(days=77)

            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ date_end
            if 'date_end' in chunk.columns and 'date_creation_processed' in chunk.columns:
                mask_no_end = chunk['date_end'].isna()
                if mask_no_end.any():
                    chunk.loc[mask_no_end, 'date_end'] = chunk.loc[mask_no_end, 'date_creation_processed'] + pd.Timedelta(days=77)

                # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                chunk['vacancy_duration'] = (chunk['date_end'] - chunk['date_creation_processed']).dt.days
                chunk['vacancy_duration'] = chunk['vacancy_duration'].fillna(77).clip(lower=1, upper=365)
                
        except Exception as e:
            print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ date_end: {e}")

        # 6. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–ª–æ–≥–∏–∫–∞ –∏–∑ improved –≤–µ—Ä—Å–∏–∏)
        initial_size = len(chunk)
        
        # –£–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –¥–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è
        if 'date_creation_processed' in chunk.columns:
            chunk = chunk[chunk['date_creation_processed'].notna()]
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏ –¥–∞—Ç (–∏–∑ improved_solution.py)
        if 'date_end' in chunk.columns and 'date_creation_processed' in chunk.columns:
            chunk = chunk[chunk['date_end'] >= chunk['date_creation_processed']]
        
        # –£–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π –æ—Ç—Ä–∞—Å–ª—å—é (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ industry)
        if 'industry' in chunk.columns:
            chunk = chunk[chunk['industry'] != 'Unknown']
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É (–æ—Å–Ω–æ–≤–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞)
        if 'date_creation_processed' in chunk.columns:
            chunk = chunk[(chunk['date_creation_processed'] >= '2018-08-01') & 
                         (chunk['date_creation_processed'] <= '2021-07-31')]

        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
        if 'date_creation_processed' in chunk.columns:
            chunk['date_creation_processed'] = pd.to_datetime(chunk['date_creation_processed'], errors='coerce')
        if 'date_end' in chunk.columns:
            chunk['date_end'] = pd.to_datetime(chunk['date_end'], errors='coerce')
        if 'work_places' in chunk.columns:
            chunk['work_places'] = pd.to_numeric(chunk['work_places'], errors='coerce').fillna(1)

        return chunk
        
    except Exception as e:
        print(f"      ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å —Ç–µ–º–∏ –∂–µ —Å—Ç–æ–ª–±—Ü–∞–º–∏
        return pd.DataFrame(columns=chunk.columns if 'chunk' in locals() else [])

def create_visualizations(df, output_dir):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    
    plt.style.use('seaborn-v0_8')
    
    # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —à–∫–∞–ª–æ–π
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.hist(df['work_places'], bins=50, alpha=0.7, edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    plt.hist(np.log1p(df['work_places']), bins=50, alpha=0.7, color='orange', edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ log(1 + —Ä–∞–±–æ—Ç–Ω—ã—Ö –º–µ—Å—Ç)')
    plt.xlabel('log(1 + –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç)')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.grid(True, alpha=0.3)
    
    # 3. –¢–æ–ø-15 –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤–∞–∫–∞–Ω—Å–∏–π
    plt.subplot(2, 2, 3)
    if 'industry' in df.columns:
        top_industries = df['industry'].value_counts().head(15)
        plt.barh(range(len(top_industries)), top_industries.values)
        plt.yticks(range(len(top_industries)), [str(ind)[:20] + '...' if len(str(ind)) > 20 else str(ind) 
                                               for ind in top_industries.index])
        plt.title('–¢–æ–ø-15 –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤–∞–∫–∞–Ω—Å–∏–π')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π')
        plt.grid(True, alpha=0.3)
    
    # 4. –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π
    plt.subplot(2, 2, 4)
    if 'date_creation_processed' in df.columns:
        monthly_counts = df.set_index('date_creation_processed').resample('M').size()
        plt.plot(monthly_counts.index, monthly_counts.values, marker='o')
        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –º–µ—Å—è—Ü–∞–º')
        plt.xlabel('–î–∞—Ç–∞')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/data_overview.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ —Å—Ä–µ–¥–Ω–µ–π –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ –∫–∞–¥—Ä–∞—Ö
    if 'industry' in df.columns:
        plt.figure(figsize=(15, 8))
        
        industry_stats = df.groupby('industry').agg({
            'work_places': ['sum', 'mean', 'count']
        }).round(2)
        
        industry_stats.columns = ['–û–±—â–∏–π_—Å–ø—Ä–æ—Å', '–°—Ä–µ–¥–Ω–∏–π_—Å–ø—Ä–æ—Å', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–≤–∞–∫–∞–Ω—Å–∏–π']
        industry_stats = industry_stats.sort_values('–û–±—â–∏–π_—Å–ø—Ä–æ—Å', ascending=False).head(20)
        
        plt.subplot(1, 2, 1)
        plt.barh(range(len(industry_stats)), industry_stats['–û–±—â–∏–π_—Å–ø—Ä–æ—Å'])
        plt.yticks(range(len(industry_stats)), 
                  [str(ind)[:25] + '...' if len(str(ind)) > 25 else str(ind) 
                   for ind in industry_stats.index])
        plt.title('–¢–æ–ø-20 –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ –æ–±—â–µ–º—É —Å–ø—Ä–æ—Å—É –Ω–∞ –∫–∞–¥—Ä—ã')
        plt.xlabel('–û–±—â–∏–π —Å–ø—Ä–æ—Å (—Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç)')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—Ç—Ä–∞—Å–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏
        stable_industries = industry_stats[industry_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–≤–∞–∫–∞–Ω—Å–∏–π'] >= 10]
        stable_industries = stable_industries.sort_values('–°—Ä–µ–¥–Ω–∏–π_—Å–ø—Ä–æ—Å', ascending=False).head(15)
        
        plt.barh(range(len(stable_industries)), stable_industries['–°—Ä–µ–¥–Ω–∏–π_—Å–ø—Ä–æ—Å'], color='orange')
        plt.yticks(range(len(stable_industries)), 
                  [str(ind)[:25] + '...' if len(str(ind)) > 25 else str(ind) 
                   for ind in stable_industries.index])
        plt.title('–¢–æ–ø-15 –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —Å–ø—Ä–æ—Å—É\n(–º–∏–Ω. 10 –≤–∞–∫–∞–Ω—Å–∏–π)')
        plt.xlabel('–°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å (—Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/industry_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. –ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å)
    salary_columns = ['base_salary_min', 'base_salary_max']
    available_salary_cols = [col for col in salary_columns if col in df.columns]
    
    if available_salary_cols:
        plt.figure(figsize=(12, 6))
        
        for i, col in enumerate(available_salary_cols):
            plt.subplot(1, len(available_salary_cols), i+1)
            
            # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            filtered_data = df[col][(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]
            
            plt.hist(filtered_data.dropna(), bins=50, alpha=0.7, edgecolor='black')
            plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col}')
            plt.xlabel('–ó–∞—Ä–ø–ª–∞—Ç–∞ (—Ä—É–±.)')
            plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/salary_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")

def prepare_vacancies_data(raw, input_file, output_file=None, visualize=True, chunk_size=25000):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏.
    –í–∫–ª—é—á–∞–µ—Ç –≤—Å—é –ª–æ–≥–∏–∫—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ improved –≤–µ—Ä—Å–∏–∏.
    """
    print("üöÄ –ù–ê–ß–ê–õ–û –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–•")
    print("="*50)
    
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if raw:
            input_file = from_raw(input_file, chunk_size=chunk_size//2)

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {input_file} (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º)...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
        if PSUTIL_AVAILABLE:
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            
            # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è 20–ì–ë —Ñ–∞–π–ª–∞ –Ω–∞ 40–ì–ë RAM
            if available_memory_gb > 30:
                chunk_size = min(chunk_size, 15000)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–∂–µ –¥–ª—è –±–æ–ª—å—à–æ–π –ø–∞–º—è—Ç–∏
            elif available_memory_gb > 20:
                chunk_size = min(chunk_size, 10000)   
            elif available_memory_gb > 15:
                chunk_size = min(chunk_size, 7500)   
            elif available_memory_gb > 10:
                chunk_size = min(chunk_size, 5000)   
            else:
                chunk_size = min(chunk_size, 2500)   # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏
            
            print(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: {available_memory_gb:.1f} –ì–ë")
            print(f"üîß –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size:,} —Å—Ç—Ä–æ–∫")
        else:
            chunk_size = min(chunk_size, 10000)  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            print(f"üîß –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {chunk_size:,} —Å—Ç—Ä–æ–∫")
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç—è–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        processed_chunks = []
        total_rows = 0
        chunk_count = 0
        
        print(f"üìä –ù–∞—á–∏–Ω–∞–µ–º —á–∞–Ω–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (—Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size:,} —Å—Ç—Ä–æ–∫)...")
        
        try:
            for chunk in pd.read_csv(input_file, delimiter=';', chunksize=chunk_size, 
                                    low_memory=False, on_bad_lines='skip'):
                chunk_count += 1
                current_chunk_size = len(chunk)
                total_rows += current_chunk_size

                
                try:
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ª–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫ —á–∞–Ω–∫—É
                    processed_chunk = process_chunk(chunk)
                    
                    if len(processed_chunk) > 0:
                        processed_chunks.append(processed_chunk)
                    else:
                        print(f"      ‚ö†Ô∏è –ß–∞–Ω–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é")
                    
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                    del chunk, processed_chunk
                    import gc
                    gc.collect()
                    
                except Exception as chunk_error:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞ {chunk_count}: {chunk_error}")
                    print(f"   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
                    continue
                    
        except Exception as read_error:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {read_error}")
            return None
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        print(f"üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ {len(processed_chunks)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤...")
        if not processed_chunks:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
            return None
            
        df = pd.concat(processed_chunks, ignore_index=True)
        
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç —á–∞–Ω–∫–æ–≤
        del processed_chunks
        import gc
        gc.collect()
        
        original_shape = (total_rows, len(df.columns))
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_rows:,} —Å—Ç—Ä–æ–∫, –∏—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {df.shape[0]:,} √ó {df.shape[1]}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ —á–∞–Ω–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        filtered_count = len(df)
        retention_rate = (filtered_count / total_rows) * 100
        print(f"üìà –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {retention_rate:.1f}% –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ work_places
        print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ work_places:")
        print(f"      –°—Ä–µ–¥–Ω–µ–µ: {df['work_places'].mean():.2f}")
        print(f"      –ú–µ–¥–∏–∞–Ω–∞: {df['work_places'].median():.2f}")
        print(f"      –ú–∞–∫—Å: {df['work_places'].max():,}")
        print(f"      99-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å: {df['work_places'].quantile(0.99):.0f}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        if visualize:
            output_dir = os.path.dirname(output_file) if output_file else '.'
            os.makedirs(output_dir, exist_ok=True)
            create_visualizations(df, output_dir)

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_shape[0]:,} √ó {original_shape[1]}")
        print(f"   –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {df.shape[0]:,} √ó {df.shape[1]}")
        print(f"   –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([col for col in df.columns if '_encoded' in col])}")
        
        if 'industry' in df.columns:
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π: {df['industry'].nunique()}")
            print(f"   –¢–æ–ø-3 –æ—Ç—Ä–∞—Å–ª–∏: {list(df['industry'].value_counts().head(3).index)}")
        
        if 'date_creation_processed' in df.columns:
            print(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {df['date_creation_processed'].min().strftime('%Y-%m-%d')} - {df['date_creation_processed'].max().strftime('%Y-%m-%d')}")
        print(f"   –û–±—â–∏–π —Å–ø—Ä–æ—Å (—Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç): {df['work_places'].sum():,}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if output_file:
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            df.to_csv(output_file, index=False, encoding='utf-8', sep=';')
            print(f"üíæ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ —Å–æ–∑–¥–∞–≤–∞–ª—Å—è
        if raw and os.path.exists(input_file) and input_file != 'vacancies_engineer.csv':
            os.remove(input_file)

        print("‚úÖ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û")
        return df

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        return None


def process_categorical_feature(df, feature):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º).
    """
    return encode_category_by_target(df, feature, drop_original=False)


if __name__ == "__main__":
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    input_file = "vacancies.csv"
    output_file = "results/processed_vacancies_engineer.csv"

    print("üéØ –ó–ê–ü–£–°–ö –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•")
    print("="*60)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
    if PSUTIL_AVAILABLE:
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è 20–ì–ë –¥–∞–Ω–Ω—ã—Ö
        if available_memory_gb > 30:
            chunk_size = 10000   
        elif available_memory_gb > 20:
            chunk_size = 7500   
        elif available_memory_gb > 15:
            chunk_size = 5000   
        elif available_memory_gb > 10:
            chunk_size = 3000   
        else:
            chunk_size = 1500   
        
        print(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: {available_memory_gb:.1f} –ì–ë")
        print(f"üîß –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size:,} —Å—Ç—Ä–æ–∫")
    else:
        chunk_size = 5000  # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        print(f"üîß –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {chunk_size:,} —Å—Ç—Ä–æ–∫")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    processed_df = prepare_vacancies_data(
        raw=True,
        input_file=input_file, 
        output_file=output_file, 
        visualize=False,
        chunk_size=chunk_size
    )

    if processed_df is not None:
        print("\nüîç –ü–†–ò–ú–ï–†–´ –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:")
        print("="*50)
        print(processed_df.head())

        print("\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ó–ê–ö–û–î–ò–†–û–í–ê–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–ê–•:")
        print("="*50)
        encoded_columns = [col for col in processed_df.columns if '_encoded' in col]
        
        for i, col in enumerate(encoded_columns[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            print(f"   {i+1:2d}. {col:30s}: {processed_df[col].min():7.2f} - {processed_df[col].max():7.2f} "
                  f"(—Å—Ä–µ–¥–Ω–µ–µ: {processed_df[col].mean():7.2f})")
        
        if len(encoded_columns) > 10:
            print(f"   ... –∏ –µ—â–µ {len(encoded_columns) - 10} –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        print(f"\nüìà –°–í–û–î–ö–ê:")
        print("="*50)
        print(f"   ‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(processed_df.columns)}")
        print(f"   üî¢ –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {len(encoded_columns)}")
        print(f"   üìä –ó–∞–ø–∏—Å–µ–π: {len(processed_df):,}")
        print(f"   üè≠ –û—Ç—Ä–∞—Å–ª–µ–π: {processed_df['industry'].nunique() if 'industry' in processed_df.columns else 'N/A'}")
        print(f"   üíº –û–±—â–∏–π —Å–ø—Ä–æ—Å: {processed_df['work_places'].sum():,} —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç")
        
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è!")
        print(f"üìÇ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")
