import pandas as pd
import numpy as np
import re
import matplotlib
matplotlib.use('Agg')  # –ö–†–ò–¢–ò–ß–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π backend –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ tkinter
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
import xgboost as xgb
import warnings
import os
from scipy.interpolate import interp1d
from scipy import stats

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
file_path = 'vacancies_engineer.csv'
output_folder = 'results'
forecast_periods = 6
validation_periods = 6

class VacancyForecaster:
    """–°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∞–Ω–æ–º–∞–ª–∏–π"""
    
    def __init__(self, output_dir='results'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.scalers = {}
        self.feature_names = None
        
    def detect_extreme_anomalies(self, ts_df, threshold_factor=2.0):
        """–ü–æ–∏—Å–∫ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π"""
        print("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π...")
        
        extreme_anomalies = {}
        
        for col in ts_df.columns:
            series = ts_df[col]
            
            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            median_val = series.median()
            q75 = series.quantile(0.75)
            q90 = series.quantile(0.90)
            q95 = series.quantile(0.95)
            max_val = series.max()
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: Z-score > 2.0
            z_scores = np.abs(stats.zscore(series))
            z_anomalies = z_scores > threshold_factor
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: –ó–Ω–∞—á–µ–Ω–∏–µ > 90-–≥–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—è –ò –±–æ–ª—å—à–µ –º–µ–¥–∏–∞–Ω—ã –≤ 2+ —Ä–∞–∑–∞
            extreme_values = (series > q90) & (series > median_val * 2)
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if col == 'total_demand':
                very_high_values = series > 80000
            else:
                very_high_values = series > (q95 * 1.2)
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –†–µ–∑–∫–∏–π —Å–∫–∞—á–æ–∫
            diff_forward = series.diff(1).abs()
            diff_backward = series.diff(-1).abs()
            spike_threshold = median_val * 1.5
            extreme_spikes = (diff_forward > spike_threshold) | (diff_backward > spike_threshold)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏
            combined_mask = z_anomalies | extreme_values | very_high_values | extreme_spikes
            
            if combined_mask.any():
                extreme_dates = series.index[combined_mask].tolist()
                extreme_values_list = series[combined_mask].tolist()
                extreme_anomalies[col] = extreme_dates
                
                print(f"  {col}: –Ω–∞–π–¥–µ–Ω–æ {len(extreme_dates)} –∞–Ω–æ–º–∞–ª–∏–π")
                for date, value in zip(extreme_dates, extreme_values_list):
                    z_score = abs((value - series.mean()) / series.std()) if series.std() > 0 else 0
            else:
                print(f"  {col}: –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        return extreme_anomalies
    
    def fix_extreme_anomalies(self, ts_df, extreme_anomalies, method='conservative_smooth', window_size=8):
        """–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¢–û–õ–¨–ö–û —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π"""
        print("üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π...")
        
        fixed_df = ts_df.copy()
        total_fixed = 0
        
        for col, anomaly_dates in extreme_anomalies.items():
            if not anomaly_dates:
                continue

                
            for anomaly_date in anomaly_dates:
                try:
                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∞–Ω–æ–º–∞–ª–∏–∏
                    anomaly_idx = fixed_df.index.get_loc(anomaly_date)
                    original_value = fixed_df.at[anomaly_date, col]
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    start_idx = max(0, anomaly_idx - window_size)
                    end_idx = min(len(fixed_df) - 1, anomaly_idx + window_size)
                    
                    # –î–∞–Ω–Ω—ã–µ –¥–æ –∏ –ø–æ—Å–ª–µ –∞–Ω–æ–º–∞–ª–∏–∏
                    before_data = fixed_df[col].iloc[start_idx:anomaly_idx]
                    after_data = fixed_df[col].iloc[anomaly_idx + 1:end_idx + 1]
                    surrounding_data = pd.concat([before_data, after_data])
                    
                    if len(surrounding_data) > 0:
                        if col == 'total_demand':
                            if original_value > 80000:
                                target_value = surrounding_data.quantile(0.75)
                                max_reduction = original_value * 0.6
                                fixed_value = int(max(target_value, max_reduction))
                            else:
                                fixed_value = int(surrounding_data.mean())
                        else:
                            if original_value > surrounding_data.quantile(0.90):
                                fixed_value = int(surrounding_data.quantile(0.90))
                            else:
                                fixed_value = int(surrounding_data.quantile(0.25))
                        
                        fixed_df.at[anomaly_date, col] = max(0, fixed_value)
                        total_fixed += 1

                        
                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ {col} –Ω–∞ –¥–∞—Ç—É {anomaly_date}: {e}")
                    continue
        
        print(f"‚úÖ –í—Å–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {total_fixed} –∞–Ω–æ–º–∞–ª–∏–π –≤ {len([col for col, dates in extreme_anomalies.items() if dates])} —Å—Ç–æ–ª–±—Ü–∞—Ö")
        return fixed_df
    
    def create_time_series(self, df, freq='M', min_activity=10):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π '{freq}'...")
        
        if len(df) == 0:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")
            return pd.DataFrame()
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        if not pd.api.types.is_datetime64_any_dtype(df['date_creation_processed']):
            df['date_creation_processed'] = pd.to_datetime(df['date_creation_processed'], errors='coerce')
        if not pd.api.types.is_datetime64_any_dtype(df['date_end']):
            df['date_end'] = pd.to_datetime(df['date_end'], errors='coerce')
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
        df = df[df['date_creation_processed'].notna() & df['date_end'].notna()]
        
        if len(df) == 0:
            print("‚ùå –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏")
            return pd.DataFrame()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        start_date = df['date_creation_processed'].min().to_period(freq).to_timestamp()
        end_date = df['date_end'].max().to_period(freq).to_timestamp()
        
        # –ü–æ–ª–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
        date_range = pd.date_range(start=start_date, end=end_date, freq=freq)
        
        if 'industry' not in df.columns:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
            ts_df = pd.DataFrame(index=date_range, columns=['total_demand']).fillna(0)
            
            for _, row in df.iterrows():
                start_period = row['date_creation_processed'].to_period(freq).to_timestamp()
                end_period = row['date_end'].to_period(freq).to_timestamp()
                active_periods = pd.date_range(start=start_period, end=end_period, freq=freq)
                
                for period in active_periods:
                    if period in ts_df.index:
                        ts_df.at[period, 'total_demand'] += row['work_places']
            
            return ts_df
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º
        industries = df['industry'].value_counts()
        active_industries = industries[industries >= min_activity].index
        
        print(f"–ê–∫—Ç–∏–≤–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π: {len(active_industries)}")
        
        if len(active_industries) == 0:
            active_industries = ['total']
            df['industry'] = 'total'
        
        ts_df = pd.DataFrame(index=date_range, columns=active_industries).fillna(0)
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        for industry in active_industries:
            industry_data = df[df['industry'] == industry] if industry != 'total' else df
            
            for _, row in industry_data.iterrows():
                start_period = row['date_creation_processed'].to_period(freq).to_timestamp()
                end_period = row['date_end'].to_period(freq).to_timestamp()
                active_periods = pd.date_range(start=start_period, end=end_period, freq=freq)
                
                for period in active_periods:
                    if period in ts_df.index:
                        ts_df.at[period, industry] += row['work_places']
        
        # –û–±—â–∏–π —Å–ø—Ä–æ—Å
        ts_df['total_demand'] = ts_df.sum(axis=1)
        
        return ts_df
    
    def remove_outliers(self, ts_df, threshold=3):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ Z-score"""
        print(f"üéØ –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ (–ø–æ—Ä–æ–≥: {threshold})...")
        
        cleaned_df = ts_df.copy()
        
        for col in cleaned_df.columns:
            series = cleaned_df[col].dropna()
            if len(series) > 3:
                z_scores = np.abs(stats.zscore(series))
                outliers = z_scores > threshold
                
                if outliers.any():
                    print(f"  {col}: –Ω–∞–π–¥–µ–Ω–æ {outliers.sum()} –≤—ã–±—Ä–æ—Å–æ–≤")
                    cleaned_series = cleaned_df[col].copy()
                    outlier_indices = series.index[outliers]
                    cleaned_series.loc[outlier_indices] = np.nan
                    cleaned_series = cleaned_series.interpolate(method='cubic')
                    cleaned_series = cleaned_series.fillna(method='bfill').fillna(method='ffill').fillna(0)
                    cleaned_df[col] = cleaned_series
        
        return cleaned_df
    
    def create_enhanced_features(self, ts, target_col, fixed_feature_set=None):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê"""
        target_series = ts[target_col] if isinstance(ts, pd.DataFrame) else ts
        target_series = target_series.fillna(method='bfill').fillna(method='ffill').fillna(0)
        
        X = pd.DataFrame(index=target_series.index)
        
        # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X['month'] = target_series.index.month
        X['quarter'] = target_series.index.quarter
        X['year'] = target_series.index.year
        X['day_of_year'] = target_series.index.dayofyear
        X['is_quarter_end'] = target_series.index.is_quarter_end.astype(int)
        X['is_year_end'] = target_series.index.is_year_end.astype(int)
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê
        max_lag = min(12, len(target_series) - 1)
        for lag in [1, 2, 3, 4, 8, 12]:
            if lag <= max_lag:
                lag_series = target_series.shift(lag)
                X[f'lag_{lag}'] = lag_series.fillna(0)  # –í–ê–ñ–ù–û: –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ!
            else:
                X[f'lag_{lag}'] = 0  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê
        for window in [3, 6, 12]:
            if window <= len(target_series):
                ma_series = target_series.rolling(window=window).mean().shift(1)  # –£–±—Ä–∞–ª–∏ min_periods=1
                X[f'ma_{window}'] = ma_series.fillna(target_series.mean())
            else:
                X[f'ma_{window}'] = target_series.mean()  # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        
        # –†–∞–∑–Ω–æ—Å—Ç–∏
        diff_1 = target_series.diff(1)
        X['diff_1'] = diff_1.fillna(0)
        
        if len(target_series) > 4:
            diff_4 = target_series.diff(4)
            X['diff_4'] = diff_4.fillna(0)
        else:
            X['diff_4'] = 0
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if fixed_feature_set is None and self.feature_names is None:
            self.feature_names = X.columns.tolist()
        
        # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–µ–º—É
        if self.feature_names is not None:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for feature in self.feature_names:
                if feature not in X.columns:
                    X[feature] = 0
            
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            X = X.reindex(columns=self.feature_names, fill_value=0)
        
        X = X.fillna(0)
        
        if target_series.isna().any():
            target_series = target_series.fillna(method='bfill').fillna(method='ffill').fillna(0)
        
        return X, target_series
    
    def train_models(self, X_train, y_train, scale_features=True):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        
        if X_train.isna().any().any():
            X_train = X_train.fillna(0)
        
        if y_train.isna().any():
            y_train = y_train.fillna(y_train.mean())
        
        if len(X_train) == 0 or len(y_train) == 0:
            raise ValueError("–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if scale_features:
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
            self.scalers['feature_scaler'] = scaler
        else:
            X_train_scaled = X_train
        
        # Random Forest
        try:
            rf_model = RandomForestRegressor(
                n_estimators=100,
                max_depth=8,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train_scaled, y_train)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Random Forest: {e}")
            rf_model = RandomForestRegressor(n_estimators=50, random_state=42)
            rf_model.fit(X_train_scaled, y_train)
        
        # XGBoost
        try:
            xgb_model = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                objective='reg:squarederror',
                verbosity=0
            )
            xgb_model.fit(X_train_scaled, y_train)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ XGBoost: {e}")
            xgb_model = xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0)
            xgb_model.fit(X_train_scaled, y_train)
        
        return rf_model, xgb_model

    def predict_with_model(self, model, X_test):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
        if 'feature_scaler' in self.scalers:
            X_test_scaled = self.scalers['feature_scaler'].transform(X_test)
            X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
        else:
            X_test_scaled = X_test
        
        return model.predict(X_test_scaled)

    def create_recursive_forecast(self, target_series, best_model_name, rf_model, xgb_model, periods=6):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        model = rf_model if best_model_name == "Random Forest" else xgb_model
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ª—É—á—à–∏—Ö –ª–∞–≥–æ–≤
        history_length = min(24, len(target_series))
        recent_values = target_series.tail(history_length).values.tolist()
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∏—Å—Ç–æ—Ä–∏—è + –ø—Ä–æ–≥–Ω–æ–∑—ã)
        all_values = recent_values.copy()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞
        forecast_values = []
        forecast_dates = []
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞
        last_date = target_series.index[-1]
        
        for i in range(periods):
            # –°–ª–µ–¥—É—é—â–∞—è –¥–∞—Ç–∞
            if hasattr(last_date, 'freq') and 'W' in str(last_date.freq):
                next_date = last_date + pd.DateOffset(weeks=i+1)
            else:
                next_date = last_date + pd.DateOffset(months=i+1)
            
            forecast_dates.append(next_date)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ - –¢–ê –ñ–ï –õ–û–ì–ò–ö–ê —á—Ç–æ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            X_next = pd.DataFrame({
                'month': [next_date.month],
                'quarter': [next_date.quarter], 
                'year': [next_date.year],
                'day_of_year': [next_date.dayofyear],
                'is_quarter_end': [1 if next_date.is_quarter_end else 0],
                'is_year_end': [1 if next_date.is_year_end else 0],
            }, index=[next_date])
            
            # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –º–∞—Å—Å–∏–≤–µ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            current_pos = len(all_values) - 1
            
            # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - –¢–ê–ö–ê–Ø –ñ–ï –õ–û–ì–ò–ö–ê –∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –Ω–æ —Å —Ä–µ–∫—É—Ä—Å–∏–µ–π
            for lag in [1, 2, 3, 4, 8, 12]:
                lag_pos = current_pos - lag + 1
                if lag_pos >= 0 and lag_pos < len(all_values):
                    X_next[f'lag_{lag}'] = [all_values[lag_pos]]
                else:
                    # –ï—Å–ª–∏ –ª–∞–≥–∞ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª–µ–º (–∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
                    X_next[f'lag_{lag}'] = [0]
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ - –¢–ê–ö–ê–Ø –ñ–ï –õ–û–ì–ò–ö–ê –∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            for window in [3, 6, 12]:
                start_pos = max(0, current_pos - window + 2)
                window_values = all_values[start_pos:]
                if len(window_values) >= window:
                    X_next[f'ma_{window}'] = [np.mean(window_values[-window:])]
                elif len(window_values) > 0:
                    # –ï—Å–ª–∏ –æ–∫–Ω–æ –Ω–µ–ø–æ–ª–Ω–æ–µ, –Ω–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                    X_next[f'ma_{window}'] = [np.mean(window_values)]
                else:
                    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ (–∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
                    X_next[f'ma_{window}'] = [np.mean(all_values) if len(all_values) > 0 else 0]
            
            # –†–∞–∑–Ω–æ—Å—Ç–∏ - –¢–ê–ö–ê–Ø –ñ–ï –õ–û–ì–ò–ö–ê –∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if len(all_values) >= 2:
                X_next['diff_1'] = [all_values[-1] - all_values[-2]]
            else:
                X_next['diff_1'] = [0]
                
            if len(all_values) >= 5:
                X_next['diff_4'] = [all_values[-1] - all_values[-5]]
            else:
                X_next['diff_4'] = [0]
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
            if self.feature_names:
                for feature in self.feature_names:
                    if feature not in X_next.columns:
                        X_next[feature] = [0]
                X_next = X_next.reindex(columns=self.feature_names, fill_value=0)
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            pred = self.predict_with_model(model, X_next)[0]
            pred = max(0, round(pred))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            all_values.append(pred)
            forecast_values.append(pred)
        
        return pd.Series(forecast_values, index=forecast_dates)

def create_industry_timeseries(df, freq='M'):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º"""
    forecaster = VacancyForecaster(output_folder)
    return forecaster.create_time_series(df, freq, min_activity=5)

def detect_and_fix_anomalies(ts_df, anomaly_dates=None, window_size=4, method='cubic', output_folder=None):
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö"""
    print(f"üîß –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –õ–û–ì–ò–ö–ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô...")
    
    forecaster = VacancyForecaster(output_folder or 'results')
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
    anomalies = forecaster.detect_extreme_anomalies(ts_df)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    if output_folder and 'total_demand' in ts_df.columns:
        try:
            plt.figure(figsize=(14, 7))
            plt.plot(ts_df.index, ts_df['total_demand'], 'b-', label='–î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è')
            
            if 'total_demand' in anomalies and anomalies['total_demand']:
                for anomaly_date in anomalies['total_demand']:
                    plt.axvline(x=anomaly_date, color='r', linestyle='--', alpha=0.7)
                    anomaly_value = ts_df.at[anomaly_date, 'total_demand']
                    plt.annotate(f'{anomaly_value:,.0f}', 
                               xy=(anomaly_date, anomaly_value), 
                               xytext=(anomaly_date, anomaly_value + 5000),
                               ha='center', fontsize=8, color='red',
                               arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))
            
            plt.title(f'–û–±—â–∏–π —Å–ø—Ä–æ—Å –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π ({ts_df.index[0].strftime("%Y-%m")} - {ts_df.index[-1].strftime("%Y-%m")})')
            plt.xlabel('–î–∞—Ç–∞')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(f'{output_folder}/anomaly_before_fix.png', dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
    fixed_df = forecaster.fix_extreme_anomalies(ts_df, anomalies, method='conservative_smooth', window_size=8)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    if output_folder and 'total_demand' in ts_df.columns:
        try:
            plt.figure(figsize=(14, 7))
            plt.plot(ts_df.index, ts_df['total_demand'], 'b-', label='–î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è', alpha=0.7)
            plt.plot(fixed_df.index, fixed_df['total_demand'], 'g-', label='–ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è')
            
            if 'total_demand' in anomalies and anomalies['total_demand']:
                for anomaly_date in anomalies['total_demand']:
                    plt.axvline(x=anomaly_date, color='r', linestyle='--', alpha=0.7)
                    original_value = ts_df.at[anomaly_date, 'total_demand']
                    fixed_value = fixed_df.at[anomaly_date, 'total_demand']
                    plt.annotate(f'{original_value:,.0f}‚Üí{fixed_value:,.0f}', 
                               xy=(anomaly_date, fixed_value), 
                               xytext=(anomaly_date, fixed_value + 8000),
                               ha='center', fontsize=8, color='darkgreen',
                               arrowprops=dict(arrowstyle='->', color='darkgreen', alpha=0.7))
            
            plt.title(f'–û–±—â–∏–π —Å–ø—Ä–æ—Å –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π ({fixed_df.index[0].strftime("%Y-%m")} - {fixed_df.index[-1].strftime("%Y-%m")})')
            plt.xlabel('–î–∞—Ç–∞')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(f'{output_folder}/anomaly_after_fix.png', dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
    
    return fixed_df

def analyze_timeseries(ts_df, output_folder):
    """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    print("–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤...")
    
    os.makedirs(output_folder, exist_ok=True)
    
    # –ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–≥–æ —Å–ø—Ä–æ—Å–∞
    plt.figure(figsize=(15, 7))
    plt.plot(ts_df.index, ts_df['total_demand'], marker='o', linestyle='-')
    plt.title('–û–±—â–∏–π —Å–ø—Ä–æ—Å –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –∫–∞–¥—Ä—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏')
    plt.xlabel('–ü–µ—Ä–∏–æ–¥')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
    plt.grid(True)
    plt.savefig(f'{output_folder}/total_demand.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –°–µ–∑–æ–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
    if len(ts_df) >= 24:
        try:
            total_demand_clean = ts_df['total_demand'].fillna(method='bfill').fillna(method='ffill').fillna(0)
            
            result = seasonal_decompose(total_demand_clean, model='additive', period=12)
            
            fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 12))
            result.observed.plot(ax=ax1)
            ax1.set_title('–ù–∞–±–ª—é–¥–∞–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            ax1.grid(True)
            
            result.trend.plot(ax=ax2)
            ax2.set_title('–¢—Ä–µ–Ω–¥')
            ax2.grid(True)
            
            result.seasonal.plot(ax=ax3)
            ax3.set_title('–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å')
            ax3.grid(True)
            
            result.resid.plot(ax=ax4)
            ax4.set_title('–û—Å—Ç–∞—Ç–∫–∏')
            ax4.grid(True)
            
            plt.tight_layout()
            plt.savefig(f'{output_folder}/seasonal_decomposition.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # –ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –º–µ—Å—è—Ü–∞–º
            seasonal_data = result.seasonal.groupby(result.seasonal.index.month).mean()
            
            plt.figure(figsize=(12, 6))
            plt.bar(seasonal_data.index, seasonal_data.values)
            plt.title('–°—Ä–µ–¥–Ω–∏–π —Å–µ–∑–æ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –ø–æ –º–µ—Å—è—Ü–∞–º')
            plt.xlabel('–ú–µ—Å—è—Ü')
            plt.ylabel('–°–µ–∑–æ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç')
            plt.grid(True, axis='y')
            plt.xticks(range(1, 13), ['–Ø–Ω–≤', '–§–µ–≤', '–ú–∞—Ä', '–ê–ø—Ä', '–ú–∞–π', '–ò—é–Ω', 
                                     '–ò—é–ª', '–ê–≤–≥', '–°–µ–Ω', '–û–∫—Ç', '–ù–æ—è', '–î–µ–∫'])
            plt.savefig(f'{output_folder}/monthly_seasonality.png', dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞: {e}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø –æ—Ç—Ä–∞—Å–ª–µ–π
    top_n = min(10, len(ts_df.columns) - 1)
    if top_n > 0:
        top_industries = ts_df.drop('total_demand', axis=1).sum().sort_values(ascending=False).head(top_n)
        
        plt.figure(figsize=(14, 8))
        sns.barplot(x=top_industries.index, y=top_industries.values)
        plt.title(f'–¢–æ–ø-{top_n} –æ—Ç—Ä–∞—Å–ª–µ–π –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
        plt.xlabel('–û—Ç—Ä–∞—Å–ª—å')
        plt.ylabel('–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(f'{output_folder}/top_industries.png', dpi=300, bbox_inches='tight')
        plt.close()
    else:
        top_industries = pd.Series(dtype=float)
    
    return top_industries

def create_features(ts, include_all_lags=True):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    forecaster = VacancyForecaster()
    
    if isinstance(ts, pd.DataFrame):
        if 'total_demand' in ts.columns:
            target_col = 'total_demand'
        else:
            target_col = ts.columns[0]
    else:
        target_col = None
    
    if target_col:
        X, y = forecaster.create_enhanced_features(ts, target_col)
    else:
        X, y = forecaster.create_enhanced_features(pd.DataFrame({'target': ts}), 'target')
        y = ts
    
    return X, y

def train_random_forest(X_train, y_train):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest"""
    forecaster = VacancyForecaster()
    rf_model, _ = forecaster.train_models(X_train, y_train)
    return rf_model

def train_gradient_boosting(X_train, y_train):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost"""
    forecaster = VacancyForecaster()
    _, xgb_model = forecaster.train_models(X_train, y_train)
    return xgb_model

def evaluate_models(y_true, rf_pred, gb_pred):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π"""
    try:
        rf_metrics = {
            'MAE': mean_absolute_error(y_true, rf_pred),
            'RMSE': np.sqrt(mean_squared_error(y_true, rf_pred)),
            'R2': r2_score(y_true, rf_pred)
        }
    except Exception as e:
        rf_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan}
    
    try:
        gb_metrics = {
            'MAE': mean_absolute_error(y_true, gb_pred),
            'RMSE': np.sqrt(mean_squared_error(y_true, gb_pred)),
            'R2': r2_score(y_true, gb_pred)
        }
    except Exception as e:
        gb_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan}
    
    return rf_metrics, gb_metrics

def visualize_forecast(history, validation, rf_forecast, gb_forecast, industry_name, rf_metrics, gb_metrics, output_folder):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞"""
    try:
        plt.figure(figsize=(15, 8))
        
        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        plt.plot(history.index, history, 'b-', label='–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ', marker='o', markersize=4)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
        plt.plot(validation.index, validation, 'g-', label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥', marker='s', markersize=4)
        
        # –ü—Ä–æ–≥–Ω–æ–∑—ã –º–æ–¥–µ–ª–µ–π
        plt.plot(validation.index, rf_forecast, 'r--', 
                 label=f'Random Forest (RMSE: {rf_metrics["RMSE"]:.2f})', marker='x', markersize=4)
        plt.plot(validation.index, gb_forecast, 'm--', 
                 label=f'XGBoost (RMSE: {gb_metrics["RMSE"]:.2f})', marker='d', markersize=4)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model = "Random Forest" if rf_metrics["RMSE"] < gb_metrics["RMSE"] else "XGBoost"
        
        plt.title(f'–ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–∞—Ö: {industry_name}\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}')
        plt.xlabel('–ü–µ—Ä–∏–æ–¥')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
        plt.grid(True)
        plt.legend(loc='best')
        
        # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        text_rf = f"Random Forest:\n"
        for metric, value in rf_metrics.items():
            text_rf += f"{metric}: {value:.2f}\n"
        
        text_gb = f"XGBoost:\n"
        for metric, value in gb_metrics.items():
            text_gb += f"{metric}: {value:.2f}\n"
        
        plt.annotate(text_rf, xy=(0.02, 0.95), xycoords='axes fraction',
                     bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))
        plt.annotate(text_gb, xy=(0.02, 0.75), xycoords='axes fraction',
                     bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))
        
        plt.tight_layout()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –¥–ª—è —Ñ–∞–π–ª–∞
        safe_name = re.sub(r'[^\w\s-]', '', industry_name).strip().replace(' ', '_')
        plt.savefig(f'{output_folder}/forecast_{safe_name}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return best_model
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è {industry_name}: {e}")
        return "XGBoost"

def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–∞—Ö"""
    print("üöÄ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø")
    print("="*60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs(output_folder, exist_ok=True)
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...")
    try:
        df = pd.read_csv(file_path, delimiter=';', low_memory=False)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {df.shape[0]} –∑–∞–ø–∏—Å–µ–π")
    except FileNotFoundError:
        alternative_paths = [
            'vacancies_engineer.csv',
            './vacancies_engineer.csv',
            '../vacancies_engineer.csv',
            'production_results/final_processed_vacancies.csv'
        ]
        
        for alt_path in alternative_paths:
            try:
                print(f"–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ {alt_path}...")
                df = pd.read_csv(alt_path, delimiter=';', low_memory=False)
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {df.shape[0]} –∑–∞–ø–∏—Å–µ–π")
                break
            except FileNotFoundError:
                continue
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
            return
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤—â–∏–∫–∞
    forecaster = VacancyForecaster(output_folder)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
    required_columns = ['date_creation_processed', 'date_end', 'work_places', 'industry']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {missing_columns}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ –º–æ–¥—É–ª–µ prepare_data_script")
        return
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ datetime
    date_columns = ['date_creation_processed', 'date_end']
    for col in date_columns:
        if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):
            print(f"   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ {col} –≤ datetime...")
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ work_places —á–∏—Å–ª–æ–≤–æ–π
    if not pd.api.types.is_numeric_dtype(df['work_places']):
        print("   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ work_places –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø...")
        df['work_places'] = pd.to_numeric(df['work_places'], errors='coerce')
        df['work_places'] = df['work_places'].fillna(1)
    
    print(f"‚úÖ –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã. –ó–∞–ø–∏—Å–µ–π —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏: {df['date_creation_processed'].notna().sum()}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    test_configs = [
        {'freq': 'W', 'outlier_removal': False},
        {'freq': 'W', 'outlier_removal': True},
        {'freq': '2W', 'outlier_removal': False},
        {'freq': 'M', 'outlier_removal': False}
    ]
    
    best_results = {'r2': -float('inf'), 'config': None, 'ts_df': None}
    
    for i, config in enumerate(test_configs):
        print(f"\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {i+1}/{len(test_configs)}: {config}")
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            ts_df = forecaster.create_time_series(df, freq=config['freq'], min_activity=5)
            
            if len(ts_df) < 20:
                print(f"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(ts_df)} —Ç–æ—á–µ–∫")
                continue
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if config['outlier_removal']:
                ts_df = forecaster.remove_outliers(ts_df)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—â–µ–≥–æ —Å–ø—Ä–æ—Å–∞
            X, y = forecaster.create_enhanced_features(ts_df, 'total_demand')
            
            if X.isna().any().any() or y.isna().any():
                print(f"  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã NaN –≤ –¥–∞–Ω–Ω—ã—Ö")
                continue
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ/—Ç–µ—Å—Ç
            train_size = int(len(X) * 0.8)
            if train_size < 10 or len(X) - train_size < 3:
                print(f"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")
                continue
            
            X_train, X_test = X[:train_size], X[train_size:]
            y_train, y_test = y[:train_size], y[train_size:]
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            rf_model, xgb_model = forecaster.train_models(X_train, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            rf_pred = forecaster.predict_with_model(rf_model, X_test)
            xgb_pred = forecaster.predict_with_model(xgb_model, X_test)
            
            # –û—Ü–µ–Ω–∫–∞
            rf_r2 = r2_score(y_test, rf_pred)
            xgb_r2 = r2_score(y_test, xgb_pred)
            best_r2 = max(rf_r2, xgb_r2)
            
            print(f"  üìà –õ—É—á—à–∏–π R¬≤: {best_r2:.4f}")
            
            if best_r2 > best_results['r2']:
                best_results = {
                    'r2': best_r2,
                    'config': config,
                    'original_ts_df': ts_df.copy(),  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    'forecaster': forecaster
                }
                print(f"  üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
            continue
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    if best_results['config']:
        print(f"\nüèÜ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_results['config']}")
        print(f"–õ—É—á—à–∏–π R¬≤: {best_results['r2']:.4f}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        original_ts = best_results['original_ts_df']
        forecaster = best_results['forecaster']
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        try:
            plt.figure(figsize=(12, 6))
            plt.plot(original_ts.index, original_ts['total_demand'], 'b-', label='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ')
            plt.title('–û–±—â–∏–π —Å–ø—Ä–æ—Å –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –∫–∞–¥—Ä—ã –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π')
            plt.xlabel('–ü–µ—Ä–∏–æ–¥')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç')
            plt.grid(True)
            plt.savefig(f'{output_folder}/demand_before_anomaly_fix.png', dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        ts_df = detect_and_fix_anomalies(original_ts, output_folder=output_folder)
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        top_industries = analyze_timeseries(ts_df, output_folder)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ª–æ–≥–∏–∫–æ–π
        print("\nüìä –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        
        forecasts = {}
        best_models = {}
        all_rf_metrics = {}
        all_gb_metrics = {}
        
        def forecast_industry(series_data, name, forecaster_instance):
            """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—Ä–∞—Å–ª–∏ —Å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ 
            X, y = forecaster_instance.create_enhanced_features(series_data, 'total_demand' if isinstance(series_data, pd.DataFrame) else None)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–∞–∫–æ–µ –∂–µ –∫–∞–∫ –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏)
            train_size = int(len(X) * 0.8)
            if train_size < 10 or len(X) - train_size < 3:
                print(f"    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ {name}")
                return None, None, None, None
            
            X_train, X_valid = X[:train_size], X[train_size:]
            y_train, y_valid = y[:train_size], y[train_size:]
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            train_data = series_data.iloc[:train_size] if isinstance(series_data, pd.DataFrame) else series_data[:train_size]
            valid_data = series_data.iloc[train_size:] if isinstance(series_data, pd.DataFrame) else series_data[train_size:]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            rf_model, xgb_model = forecaster_instance.train_models(X_train, y_train)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            rf_pred = forecaster_instance.predict_with_model(rf_model, X_valid)
            xgb_pred = forecaster_instance.predict_with_model(xgb_model, X_valid)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            rf_metrics = {
                'MAE': mean_absolute_error(y_valid, rf_pred),
                'RMSE': np.sqrt(mean_squared_error(y_valid, rf_pred)),
                'R2': r2_score(y_valid, rf_pred)
            }
            
            xgb_metrics = {
                'MAE': mean_absolute_error(y_valid, xgb_pred),
                'RMSE': np.sqrt(mean_squared_error(y_valid, xgb_pred)),
                'R2': r2_score(y_valid, xgb_pred)
            }
            
            best_model = "Random Forest" if rf_metrics["RMSE"] < xgb_metrics["RMSE"] else "XGBoost"
            
            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
            print(f"    –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è {name}...")
            
            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö, –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞—è feature_names
            X_full, y_full = forecaster_instance.create_enhanced_features(series_data, 'total_demand' if isinstance(series_data, pd.DataFrame) else None)
            rf_final, xgb_final = forecaster_instance.train_models(X_full, y_full)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑
            target_series = series_data if not isinstance(series_data, pd.DataFrame) else series_data['total_demand']
            final_forecast = forecaster_instance.create_recursive_forecast(
                target_series, best_model, rf_final, xgb_final, forecast_periods
            )
            
            return rf_metrics, xgb_metrics, best_model, final_forecast, train_data, valid_data, rf_pred, xgb_pred
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–ø—Ä–æ—Å–∞
        print("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–ø—Ä–æ—Å–∞...")
        total_demand = ts_df['total_demand']
        
        # –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞–µ–º feature_names - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —á—Ç–æ —Å–æ–∑–¥–∞–ª–∏—Å—å –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
        # forecaster.feature_names –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
        
        result = forecast_industry(total_demand, 'total_demand', forecaster)
        
        if result[0] is not None:
            rf_metrics, xgb_metrics, best_model, final_forecast, train_data, valid_data, rf_pred, xgb_pred = result
            
            print(f"‚úÖ –û–±—â–∏–π —Å–ø—Ä–æ—Å - Random Forest R¬≤: {rf_metrics['R2']:.4f}, XGBoost R¬≤: {xgb_metrics['R2']:.4f}")
            print(f"    –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑: {final_forecast.iloc[0]:,.0f} - {final_forecast.iloc[-1]:,.0f}")
            
            all_rf_metrics['total_demand'] = rf_metrics
            all_gb_metrics['total_demand'] = xgb_metrics
            best_models['total_demand'] = best_model
            forecasts['total_demand'] = final_forecast
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            visualize_forecast(train_data, valid_data, rf_pred, xgb_pred, '–û–±—â–∏–π —Å–ø—Ä–æ—Å', rf_metrics, xgb_metrics, output_folder)
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ–ø-–æ—Ç—Ä–∞—Å–ª–µ–π
        for industry in top_industries.index[:5]:
            try:
                print(f"–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—Ä–∞—Å–ª–∏: {industry}")
                industry_data = ts_df[industry]
                
                if len(industry_data) > 24:
                    # –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞–µ–º feature_names - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    # forecaster.feature_names = None
                    
                    result = forecast_industry(industry_data, industry, forecaster)
                    
                    if result[0] is not None:
                        rf_metrics, xgb_metrics, best_model, final_forecast, train_data, valid_data, rf_pred, xgb_pred = result
                        
                        print(f"‚úÖ {industry} - Random Forest R¬≤: {rf_metrics['R2']:.4f}, XGBoost R¬≤: {xgb_metrics['R2']:.4f}")
                        print(f"    –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑: {final_forecast.iloc[0]:,.0f} - {final_forecast.iloc[-1]:,.0f}")
                        
                        all_rf_metrics[industry] = rf_metrics
                        all_gb_metrics[industry] = xgb_metrics
                        best_models[industry] = best_model
                        forecasts[industry] = final_forecast
                        
                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                        visualize_forecast(train_data, valid_data, rf_pred, xgb_pred, industry, rf_metrics, xgb_metrics, output_folder)
                else:
                    print(f"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—Ä–∞—Å–ª–∏ {industry}: {len(industry_data)} —Ç–æ—á–µ–∫")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª—è –æ—Ç—Ä–∞—Å–ª–∏ {industry}: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if forecasts:
            # DataFrame —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏
            forecast_df = pd.DataFrame(forecasts)
            forecast_df.to_csv(f'{output_folder}/industry_demand_forecast.csv')
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            if all_rf_metrics:
                rf_metrics_summary = pd.DataFrame.from_dict(all_rf_metrics, orient='index')
                rf_metrics_summary.to_csv(f'{output_folder}/random_forest_metrics.csv')
            
            if all_gb_metrics:
                gb_metrics_summary = pd.DataFrame.from_dict(all_gb_metrics, orient='index')
                gb_metrics_summary.to_csv(f'{output_folder}/gradient_boosting_metrics.csv')
            
            # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
            if best_models:
                pd.Series(best_models).to_csv(f'{output_folder}/best_models_per_industry.csv')
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            if all_rf_metrics and all_gb_metrics:
                try:
                    plt.figure(figsize=(12, 8))
                    metrics = ['MAE', 'RMSE', 'R2']
                    x = range(len(metrics))
                    width = 0.35
                    
                    rf_values = [rf_metrics_summary[metric].mean(skipna=True) for metric in metrics]
                    gb_values = [gb_metrics_summary[metric].mean(skipna=True) for metric in metrics]
                    
                    plt.bar([i - width/2 for i in x], rf_values, width, label='Random Forest')
                    plt.bar([i + width/2 for i in x], gb_values, width, label='XGBoost')
                    
                    plt.xlabel('–ú–µ—Ç—Ä–∏–∫–∞')
                    plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
                    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–µ–¥–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º')
                    plt.xticks(x, metrics)
                    plt.legend()
                    plt.savefig(f'{output_folder}/model_comparison.png', dpi=300, bbox_inches='tight')
                    plt.close()
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            rf_count = list(best_models.values()).count("Random Forest")
            gb_count = list(best_models.values()).count("XGBoost")
            
            if all_rf_metrics and all_gb_metrics:
                overall_best_model = "Random Forest" if rf_metrics_summary['RMSE'].mean(skipna=True) < gb_metrics_summary['RMSE'].mean(skipna=True) else "XGBoost"
                
                report = f"""
–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–∞—Ö:

1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –ø–µ—Ä–∏–æ–¥: {ts_df.index[0].strftime('%Y-%m')} - {ts_df.index[-1].strftime('%Y-%m')}
2. –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df)}
3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ –æ—Ç—Ä–∞—Å–ª–∏: {len(top_industries)}
4. –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_results['config']}
5. –õ—É—á—à–∏–π R¬≤ (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ): {best_results['r2']:.4f}

6. –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π:
   Random Forest:
   - MAE: {rf_metrics_summary['MAE'].mean(skipna=True):.2f}
   - RMSE: {rf_metrics_summary['RMSE'].mean(skipna=True):.2f}
   - R2: {rf_metrics_summary['R2'].mean(skipna=True):.4f}
   
   XGBoost:
   - MAE: {gb_metrics_summary['MAE'].mean(skipna=True):.2f}
   - RMSE: {gb_metrics_summary['RMSE'].mean(skipna=True):.2f}
   - R2: {gb_metrics_summary['R2'].mean(skipna=True):.4f}

7. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º:
   Random Forest: {rf_count} –æ—Ç—Ä–∞—Å–ª–µ–π ({rf_count/(rf_count+gb_count)*100 if rf_count+gb_count > 0 else 0:.1f}%)
   XGBoost: {gb_count} –æ—Ç—Ä–∞—Å–ª–µ–π ({gb_count/(rf_count+gb_count)*100 if rf_count+gb_count > 0 else 0:.1f}%)

8. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É RMSE: {overall_best_model}

9. –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {forecast_periods} –ø–µ—Ä–∏–æ–¥–æ–≤

–§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã —Å–æ–∑–¥–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π –ª–æ–≥–∏–∫–∏ –∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
"""
                
                print(report)
                
                with open(f'{output_folder}/final_report.txt', 'w', encoding='utf-8') as f:
                    f.write(report)
            
            print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É {output_folder}")
            print(f"üéØ –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ {forecast_periods} –ø–µ—Ä–∏–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω—ã —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π!")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")

if __name__ == "__main__":
    main()